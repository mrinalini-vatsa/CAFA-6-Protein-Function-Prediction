{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T12:38:42.155615Z","iopub.execute_input":"2025-11-18T12:38:42.155783Z","iopub.status.idle":"2025-11-18T12:38:43.820593Z","shell.execute_reply.started":"2025-11-18T12:38:42.155767Z","shell.execute_reply":"2025-11-18T12:38:43.819830Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\n/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\n/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 1 - Install Dependencies\n\nimport sys\n!{sys.executable} -m pip install transformers biopython scipy -q\n\n# Verify installation\ntry:\n    from Bio import SeqIO\n    from transformers import AutoModel\n    import scipy.sparse as sp\n    print(\"‚úÖ All dependencies installed successfully!\")\n    print(\"   - transformers\")\n    print(\"   - biopython\") \n    print(\"   - scipy\")\n    print(\"\\n‚úÖ Now run CELL 2\")\nexcept ImportError as e:\n    print(f\"‚ùå Installation failed: {e}\")\n    print(\"   Try restarting the kernel and running this cell again\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T12:39:06.730673Z","iopub.execute_input":"2025-11-18T12:39:06.730934Z","iopub.status.idle":"2025-11-18T12:39:22.748687Z","shell.execute_reply.started":"2025-11-18T12:39:06.730913Z","shell.execute_reply":"2025-11-18T12:39:22.747996Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h‚úÖ All dependencies installed successfully!\n   - transformers\n   - biopython\n   - scipy\n\n‚úÖ Now run CELL 2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nCELL 2 - LOAD AND PREPARE DATA\nThis loads sequences, annotations, and creates datasets\nTime: ~2-3 minutes\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom Bio import SeqIO\nfrom collections import defaultdict\nimport scipy.sparse as sp\nimport gc\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=\"*70)\nprint(\"üìä CELL 2: LOADING DATA\")\nprint(\"=\"*70)\n\n# Path to data\nDATA_PATH = '/kaggle/input/cafa-6-protein-function-prediction'\n\n# ----------------------------------------------------------------------------\n# 2.1: Load sequences with correct ID parsing\n# ----------------------------------------------------------------------------\n\ndef parse_uniprot_id(fasta_header):\n    \"\"\"Extract UniProt accession: 'sp|P9WHI7|RECN_MYCT' -> 'P9WHI7'\"\"\"\n    parts = fasta_header.split('|')\n    return parts[1] if len(parts) >= 2 else fasta_header\n\ndef load_fasta(filepath, max_samples=None):\n    \"\"\"Load FASTA file\"\"\"\n    records = []\n    print(f\"Loading {filepath}...\")\n    for i, record in enumerate(SeqIO.parse(filepath, \"fasta\")):\n        records.append({\n            'protein_id': parse_uniprot_id(record.id),\n            'sequence': str(record.seq)\n        })\n        if max_samples and i >= max_samples - 1:\n            break\n    return pd.DataFrame(records)\n\n# Load training sequences (use subset for faster training)\nUSE_SUBSET = False  # Set False for full training\nif USE_SUBSET:\n    print(\"‚ö†Ô∏è Using 10k subset for faster training\")\n    train_sequences = load_fasta(f'{DATA_PATH}/Train/train_sequences.fasta', max_samples=10000)\nelse:\n    train_sequences = load_fasta(f'{DATA_PATH}/Train/train_sequences.fasta')\n\nprint(f\"‚úÖ Train sequences: {len(train_sequences):,}\")\n\n# ----------------------------------------------------------------------------\n# 2.2: Load GO annotations\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüìã Loading GO annotations...\")\ntrain_annotations = pd.read_csv(\n    f'{DATA_PATH}/Train/train_terms.tsv',\n    sep='\\t', header=None,\n    names=['protein_id', 'go_term', 'aspect']\n)\n\n# Filter to loaded proteins\ntrain_protein_ids = set(train_sequences['protein_id'].values)\ntrain_annotations = train_annotations[train_annotations['protein_id'].isin(train_protein_ids)]\n\nprint(f\"‚úÖ Annotations: {len(train_annotations):,}\")\n\n# ----------------------------------------------------------------------------\n# 2.3: Filter GO terms by frequency\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüî§ Filtering GO terms...\")\nMIN_FREQ = 10  # Terms must appear at least 10 times\nterm_counts = train_annotations['go_term'].value_counts()\nfrequent_terms = term_counts[term_counts >= MIN_FREQ].index.tolist()\ntrain_annotations = train_annotations[train_annotations['go_term'].isin(frequent_terms)]\n\nall_go_terms = sorted(frequent_terms)\nprint(f\"‚úÖ Using {len(all_go_terms):,} GO terms\")\n\n# ----------------------------------------------------------------------------\n# 2.4: Create sparse label matrix\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüî¢ Creating label matrix...\")\ndef create_sparse_labels(annotations_df, protein_ids, go_terms):\n    n_proteins, n_terms = len(protein_ids), len(go_terms)\n    protein_to_idx = {pid: i for i, pid in enumerate(protein_ids)}\n    term_to_idx = {term: i for i, term in enumerate(go_terms)}\n    \n    label_matrix = sp.lil_matrix((n_proteins, n_terms), dtype=np.float32)\n    \n    for _, row in annotations_df.iterrows():\n        if row['protein_id'] in protein_to_idx and row['go_term'] in term_to_idx:\n            label_matrix[protein_to_idx[row['protein_id']], term_to_idx[row['go_term']]] = 1.0\n    \n    return label_matrix.tocsr()\n\ntrain_labels_sparse = create_sparse_labels(\n    train_annotations,\n    train_sequences['protein_id'].tolist(),\n    all_go_terms\n)\nprint(f\"‚úÖ Label matrix: {train_labels_sparse.shape}\")\n\n# ----------------------------------------------------------------------------\n# 2.5: Train/validation split\n# ----------------------------------------------------------------------------\n\nprint(\"\\n‚úÇÔ∏è Splitting data...\")\ntrain_indices, val_indices = train_test_split(\n    np.arange(len(train_sequences)), test_size=0.15, random_state=42\n)\n\ntrain_df = train_sequences.iloc[train_indices].reset_index(drop=True)\nval_df = train_sequences.iloc[val_indices].reset_index(drop=True)\ntrain_labels_split = train_labels_sparse[train_indices]\nval_labels_split = train_labels_sparse[val_indices]\n\nprint(f\"‚úÖ Train: {len(train_df):,} | Val: {len(val_df):,}\")\n\n# ----------------------------------------------------------------------------\n# 2.6: Create datasets\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüîß Creating datasets...\")\n\nclass SparseProteinDataset(Dataset):\n    def __init__(self, sequences_df, labels_sparse, go_terms):\n        self.sequences_df = sequences_df.reset_index(drop=True)\n        self.labels_sparse = labels_sparse\n        self.go_terms = go_terms\n    \n    def __len__(self):\n        return len(self.sequences_df)\n    \n    def __getitem__(self, idx):\n        row = self.sequences_df.iloc[idx]\n        labels_dense = self.labels_sparse[idx].toarray().squeeze() if self.labels_sparse is not None else None\n        return {\n            'sequence': row['sequence'],\n            'protein_id': row['protein_id'],\n            'labels': torch.FloatTensor(labels_dense) if labels_dense is not None else None\n        }\n\ntrain_dataset = SparseProteinDataset(train_df, train_labels_split, all_go_terms)\nval_dataset = SparseProteinDataset(val_df, val_labels_split, all_go_terms)\n\nBATCH_SIZE = 4\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nprint(f\"‚úÖ DataLoaders ready (batch size: {BATCH_SIZE})\")\n\n# ----------------------------------------------------------------------------\n# 2.7: Load GO hierarchy and IA weights\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüå≥ Loading GO hierarchy...\")\n# Simplified hierarchy (each term is its own parent)\ngo_hierarchy_df = pd.DataFrame({'child': all_go_terms, 'parent': all_go_terms})\n\nprint(\"\\n‚öñÔ∏è Loading IA weights...\")\nia_df = pd.read_csv(f'{DATA_PATH}/IA.tsv', sep='\\t', header=None, names=['go_term', 'ia_weight'])\nia_df = ia_df[ia_df['go_term'].isin(all_go_terms)]\nia_weights_dict = dict(zip(ia_df['go_term'], ia_df['ia_weight']))\n\n# Fill missing with mean\nmean_ia = np.mean(list(ia_weights_dict.values()))\nfor term in all_go_terms:\n    if term not in ia_weights_dict:\n        ia_weights_dict[term] = mean_ia\n\nprint(f\"‚úÖ IA weights loaded\")\n\n# Cleanup\ndel train_annotations, train_labels_sparse\ngc.collect()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ CELL 2 COMPLETE - Data loaded and ready!\")\nprint(\"=\"*70)\nprint(\"\\nNext: Run CELL 3 (Model Definition)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T12:39:59.380446Z","iopub.execute_input":"2025-11-18T12:39:59.381386Z","iopub.status.idle":"2025-11-18T12:40:25.285229Z","shell.execute_reply.started":"2025-11-18T12:39:59.381357Z","shell.execute_reply":"2025-11-18T12:40:25.284491Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä CELL 2: LOADING DATA\n======================================================================\nLoading /kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta...\n‚úÖ Train sequences: 82,404\n\nüìã Loading GO annotations...\n‚úÖ Annotations: 537,027\n\nüî§ Filtering GO terms...\n‚úÖ Using 7,873 GO terms\n\nüî¢ Creating label matrix...\n‚úÖ Label matrix: (82404, 7873)\n\n‚úÇÔ∏è Splitting data...\n‚úÖ Train: 70,043 | Val: 12,361\n\nüîß Creating datasets...\n‚úÖ DataLoaders ready (batch size: 4)\n\nüå≥ Loading GO hierarchy...\n\n‚öñÔ∏è Loading IA weights...\n‚úÖ IA weights loaded\n\n======================================================================\n‚úÖ CELL 2 COMPLETE - Data loaded and ready!\n======================================================================\n\nNext: Run CELL 3 (Model Definition)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nCELL 3 - MODEL DEFINITION\nThis defines the neural network and helper classes\nTime: ~1 minute\n\"\"\"\n\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\n\nprint(\"=\"*70)\nprint(\"üß† CELL 3: DEFINING MODEL\")\nprint(\"=\"*70)\n\n# ----------------------------------------------------------------------------\n# 3.1: GO Hierarchy Class\n# ----------------------------------------------------------------------------\n\nclass GOHierarchy:\n    \"\"\"Handles GO term hierarchy for label propagation\"\"\"\n    def __init__(self, hierarchy_df, all_go_terms):\n        self.all_go_terms = all_go_terms\n        self.term_to_idx = {term: i for i, term in enumerate(all_go_terms)}\n        self.children = {term: set() for term in all_go_terms}\n        self.parents = {term: set() for term in all_go_terms}\n        \n        for _, row in hierarchy_df.iterrows():\n            child, parent = row['child'], row['parent']\n            if child in self.term_to_idx and parent in self.term_to_idx:\n                self.children[parent].add(child)\n                self.parents[child].add(parent)\n    \n    def propagate_predictions(self, predictions):\n        \"\"\"Propagate child probabilities to parents\"\"\"\n        if isinstance(predictions, torch.Tensor):\n            pred_np = predictions.cpu().numpy()\n            was_torch = True\n        else:\n            pred_np = predictions\n            was_torch = False\n        \n        propagated = pred_np.copy()\n        for term in self.all_go_terms:\n            if term not in self.parents:\n                continue\n            term_idx = self.term_to_idx[term]\n            term_probs = propagated[:, term_idx]\n            for parent in self.parents[term]:\n                parent_idx = self.term_to_idx[parent]\n                propagated[:, parent_idx] = np.maximum(propagated[:, parent_idx], term_probs)\n        \n        return torch.from_numpy(propagated).to(predictions.device) if was_torch else propagated\n\nprint(\"‚úÖ GOHierarchy class defined\")\n\n# ----------------------------------------------------------------------------\n# 3.2: IA Weights Class\n# ----------------------------------------------------------------------------\n\nclass IAWeights:\n    \"\"\"Handles Information Accretion weights\"\"\"\n    def __init__(self, ia_dict, all_go_terms):\n        self.weights = torch.FloatTensor([ia_dict.get(term, 1.0) for term in all_go_terms])\n    \n    def get_weights(self, device='cpu'):\n        return self.weights.to(device)\n\nprint(\"‚úÖ IAWeights class defined\")\n\n# ----------------------------------------------------------------------------\n# 3.3: Main Model Architecture\n# ----------------------------------------------------------------------------\n\nclass ProteinFunctionPredictor(nn.Module):\n    \"\"\"ESM2-based protein function predictor\"\"\"\n    def __init__(self, num_go_terms, esm_model_name=\"facebook/esm2_t33_650M_UR50D\",\n                 hidden_dim=1024, dropout=0.3, freeze_esm=True):\n        super().__init__()\n        self.num_go_terms = num_go_terms\n        self.esm_model_name = esm_model_name\n        \n        # Load ESM2\n        self.esm = AutoModel.from_pretrained(esm_model_name)\n        if freeze_esm:\n            for param in self.esm.parameters():\n                param.requires_grad = False\n        \n        self.embedding_dim = self.esm.config.hidden_size\n        \n        # Classifier head\n        self.classifier = nn.Sequential(\n            nn.Linear(self.embedding_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, num_go_terms)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad() if not self.training else torch.enable_grad():\n            outputs = self.esm(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_embedding = outputs.last_hidden_state[:, 0, :]\n        logits = self.classifier(sequence_embedding)\n        return logits\n\nprint(\"‚úÖ ProteinFunctionPredictor class defined\")\n\n# ----------------------------------------------------------------------------\n# 3.4: Trainer Class\n# ----------------------------------------------------------------------------\n\nclass CAFA6Trainer:\n    \"\"\"Handles training loop\"\"\"\n    def __init__(self, model, go_hierarchy, ia_weights, device='cuda', learning_rate=2e-4):\n        self.model = model.to(device)\n        self.go_hierarchy = go_hierarchy\n        self.ia_weights = ia_weights\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model.esm_model_name)\n        \n        weights = ia_weights.get_weights(device) / ia_weights.get_weights(device).mean()\n        self.criterion = nn.BCEWithLogitsLoss(pos_weight=weights)\n        self.optimizer = torch.optim.AdamW(\n            [p for p in model.parameters() if p.requires_grad],\n            lr=learning_rate, weight_decay=0.01\n        )\n\nprint(\"‚úÖ CAFA6Trainer class defined\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ CELL 3 COMPLETE - Model architecture defined!\")\nprint(\"=\"*70)\nprint(\"\\nNext: Run CELL 4 (Training)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T12:40:51.022700Z","iopub.execute_input":"2025-11-18T12:40:51.023430Z","iopub.status.idle":"2025-11-18T12:40:51.038055Z","shell.execute_reply.started":"2025-11-18T12:40:51.023403Z","shell.execute_reply":"2025-11-18T12:40:51.037430Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüß† CELL 3: DEFINING MODEL\n======================================================================\n‚úÖ GOHierarchy class defined\n‚úÖ IAWeights class defined\n‚úÖ ProteinFunctionPredictor class defined\n‚úÖ CAFA6Trainer class defined\n\n======================================================================\n‚úÖ CELL 3 COMPLETE - Model architecture defined!\n======================================================================\n\nNext: Run CELL 4 (Training)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nCELL 4 - TRAIN MODEL\nThis trains the model on training data\nTime: ~10-15 minutes for 3 epochs\n\"\"\"\nfrom tqdm.auto import tqdm\n\nprint(\"=\"*70)\nprint(\"üèãÔ∏è CELL 4: TRAINING MODEL\")\nprint(\"=\"*70)\n\n# ----------------------------------------------------------------------------\n# 4.1: Initialize components\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüîß Initializing...\")\ngo_hierarchy = GOHierarchy(go_hierarchy_df, all_go_terms)\nia_weights = IAWeights(ia_weights_dict, all_go_terms)\n\nmodel = ProteinFunctionPredictor(\n    num_go_terms=len(all_go_terms),\n    esm_model_name=\"facebook/esm2_t12_35M_UR50D\",\n    hidden_dim=512, dropout=0.3, freeze_esm=True\n)\n\ntrainer = CAFA6Trainer(\n    model=model, go_hierarchy=go_hierarchy, ia_weights=ia_weights,\n    device='cuda' if torch.cuda.is_available() else 'cpu', learning_rate=2e-4\n)\n\nprint(f\"‚úÖ Model ready: {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable params\")\n\n# ----------------------------------------------------------------------------\n# 4.2: Training configuration\n# ----------------------------------------------------------------------------\n\nNUM_EPOCHS = 8\nACCUMULATION_STEPS = 4\nMAX_SEQ_LENGTH = 512\n\nprint(f\"\\n‚öôÔ∏è Config: {NUM_EPOCHS} epochs, batch={BATCH_SIZE}, accumulation={ACCUMULATION_STEPS}\")\n\n# ----------------------------------------------------------------------------\n# 4.3: Training loop\n# ----------------------------------------------------------------------------\n\nbest_val_loss = float('inf')\ntraining_history = {'train_loss': [], 'val_loss': []}\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\n{'='*70}\")\n    print(f\"EPOCH {epoch+1}/{NUM_EPOCHS}\")\n    print('='*70)\n    \n    # Training\n    model.train()\n    total_loss = 0\n    batch_count = 0\n    trainer.optimizer.zero_grad()\n    \n    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n        sequences = batch['sequence']\n        labels = batch['labels'].to(trainer.device)\n        \n        encoded = trainer.tokenizer(sequences, padding=True, truncation=True,\n                                   max_length=MAX_SEQ_LENGTH, return_tensors='pt')\n        input_ids = encoded['input_ids'].to(trainer.device)\n        attention_mask = encoded['attention_mask'].to(trainer.device)\n        \n        logits = model(input_ids, attention_mask)\n        loss = trainer.criterion(logits, labels) / ACCUMULATION_STEPS\n        loss.backward()\n        \n        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            trainer.optimizer.step()\n            trainer.optimizer.zero_grad()\n        \n        total_loss += loss.item() * ACCUMULATION_STEPS\n        batch_count += 1\n        \n        del input_ids, attention_mask, logits, loss, encoded\n        if batch_idx % 20 == 0:\n            torch.cuda.empty_cache()\n            gc.collect()\n    \n    avg_train_loss = total_loss / batch_count\n    training_history['train_loss'].append(avg_train_loss)\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    val_batch_count = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            sequences = batch['sequence']\n            labels = batch['labels'].to(trainer.device)\n            \n            encoded = trainer.tokenizer(sequences, padding=True, truncation=True,\n                                       max_length=MAX_SEQ_LENGTH, return_tensors='pt')\n            input_ids = encoded['input_ids'].to(trainer.device)\n            attention_mask = encoded['attention_mask'].to(trainer.device)\n            \n            logits = model(input_ids, attention_mask)\n            loss = trainer.criterion(logits, labels)\n            val_loss += loss.item()\n            val_batch_count += 1\n            \n            del input_ids, attention_mask, logits, loss, encoded\n    \n    val_loss /= val_batch_count\n    training_history['val_loss'].append(val_loss)\n    \n    print(f\"\\nüìà Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': trainer.optimizer.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': val_loss,\n            'go_terms': all_go_terms\n        }, '/kaggle/working/best_model.pt')\n        print(f\"‚úÖ Best model saved! (Val Loss: {val_loss:.4f})\")\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n\n# Load best model\ncheckpoint = torch.load('/kaggle/working/best_model.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"\\n‚úÖ Loaded best model from epoch {checkpoint['epoch']+1}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ CELL 4 COMPLETE - Model trained!\")\nprint(\"=\"*70)\nprint(\"\\nNext: Run CELL 5 (Generate Predictions)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T13:27:03.681940Z","iopub.execute_input":"2025-11-18T13:27:03.682572Z","iopub.status.idle":"2025-11-18T19:05:36.092676Z","shell.execute_reply.started":"2025-11-18T13:27:03.682550Z","shell.execute_reply":"2025-11-18T19:05:36.092012Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüèãÔ∏è CELL 4: TRAINING MODEL\n======================================================================\n\nüîß Initializing...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Model ready: 2,400,961 trainable params\n\n‚öôÔ∏è Config: 8 epochs, batch=4, accumulation=4\n\n======================================================================\nEPOCH 1/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b18c5637d204ae69ea24bc7729c134d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b69213680e2433198acd0a61800084e"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 1: Train Loss=0.0084, Val Loss=0.0038\n‚úÖ Best model saved! (Val Loss: 0.0038)\n\n======================================================================\nEPOCH 2/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2739e09b084d40ba85aa5e0a6982e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8dd51079034539a27fa222ae9313f4"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 2: Train Loss=0.0039, Val Loss=0.0037\n‚úÖ Best model saved! (Val Loss: 0.0037)\n\n======================================================================\nEPOCH 3/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcca8cdbc8da42fcbc6e742d86da879f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a1f4f7032748f4a0695cdeaf72785a"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 3: Train Loss=0.0037, Val Loss=0.0036\n‚úÖ Best model saved! (Val Loss: 0.0036)\n\n======================================================================\nEPOCH 4/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7de35b51b54153b90d16ab02af6bb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59f6ac300d32484299e66b7edb1c644e"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 4: Train Loss=0.0036, Val Loss=0.0035\n‚úÖ Best model saved! (Val Loss: 0.0035)\n\n======================================================================\nEPOCH 5/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77a2c191617a4afc94b6ee5c37831d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5164966c12184a27a8b2639fb45b066d"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 5: Train Loss=0.0035, Val Loss=0.0034\n‚úÖ Best model saved! (Val Loss: 0.0034)\n\n======================================================================\nEPOCH 6/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179175639c9d44ffb3fdb27c4b5f5e27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3992d52d0cbc447b8bddb73c71bd9170"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 6: Train Loss=0.0034, Val Loss=0.0034\n‚úÖ Best model saved! (Val Loss: 0.0034)\n\n======================================================================\nEPOCH 7/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378af00117564a38b6392b2bc7afb2a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0bdf79c4a3f436dbeee60ea5886b77a"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 7: Train Loss=0.0034, Val Loss=0.0033\n‚úÖ Best model saved! (Val Loss: 0.0033)\n\n======================================================================\nEPOCH 8/8\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/17511 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7feb681a824e84832d14be3d00a48e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09ac1a32dbb44f899e54aa1b11049d2"}},"metadata":{}},{"name":"stdout","text":"\nüìà Epoch 8: Train Loss=0.0033, Val Loss=0.0033\n‚úÖ Best model saved! (Val Loss: 0.0033)\n\n‚úÖ Loaded best model from epoch 8\n\n======================================================================\n‚úÖ CELL 4 COMPLETE - Model trained!\n======================================================================\n\nNext: Run CELL 5 (Generate Predictions)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\"\"\"\nCELL 5 - GENERATE PREDICTIONS FOR ALL TEST PROTEINS\nThis is the CRITICAL cell - predicts for all 224k proteins\nTime: ~60-90 minutes\n\"\"\"\n\nprint(\"=\"*70)\nprint(\"üîÆ CELL 5: GENERATING PREDICTIONS FOR ALL TEST PROTEINS\")\nprint(\"=\"*70)\n\n# ----------------------------------------------------------------------------\n# 5.1: Load FULL test superset\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüì• Loading FULL test superset...\")\nprint(\"‚ö†Ô∏è Loading ALL 224,309 proteins (not just 5,000)\")\n\ntest_sequences_full = []\nfor record in SeqIO.parse(f'{DATA_PATH}/Test/testsuperset.fasta', 'fasta'):\n    test_sequences_full.append({\n        'protein_id': parse_uniprot_id(record.id),\n        'sequence': str(record.seq)\n    })\n\ntest_df_full = pd.DataFrame(test_sequences_full)\nprint(f\"‚úÖ Loaded {len(test_df_full):,} test proteins\")\n\n# ----------------------------------------------------------------------------\n# 5.2: Create test dataset and loader\n# ----------------------------------------------------------------------------\n\nclass TestProteinDataset(Dataset):\n    def __init__(self, sequences_df):\n        self.sequences_df = sequences_df.reset_index(drop=True)\n    \n    def __len__(self):\n        return len(self.sequences_df)\n    \n    def __getitem__(self, idx):\n        row = self.sequences_df.iloc[idx]\n        return {\n            'sequence': row['sequence'],\n            'protein_id': row['protein_id']\n        }\n\ntest_dataset_full = TestProteinDataset(test_df_full)\ntest_loader_full = DataLoader(test_dataset_full, batch_size=4, shuffle=False, num_workers=0)\n\nprint(f\"‚úÖ Test loader ready: {len(test_dataset_full):,} proteins\")\n\n# ----------------------------------------------------------------------------\n# 5.3: Generate predictions (THIS TAKES TIME!)\n# ----------------------------------------------------------------------------\n\nprint(\"\\nüîÆ Generating predictions...\")\nprint(\"‚è±Ô∏è This will take 60-90 minutes - be patient!\")\n\nmodel.eval()\nall_predictions_full = []\nall_protein_ids_full = []\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(tqdm(test_loader_full, desc=\"Predicting\")):\n        sequences = batch['sequence']\n        protein_ids = batch['protein_id']\n        \n        # Tokenize\n        encoded = trainer.tokenizer(\n            sequences, padding=True, truncation=True,\n            max_length=MAX_SEQ_LENGTH, return_tensors='pt'\n        )\n        input_ids = encoded['input_ids'].to(trainer.device)\n        attention_mask = encoded['attention_mask'].to(trainer.device)\n        \n        # Predict\n        logits = model(input_ids, attention_mask)\n        probs = torch.sigmoid(logits)\n        \n        # Propagate through GO hierarchy\n        probs = go_hierarchy.propagate_predictions(probs)\n        \n        all_predictions_full.append(probs.cpu().numpy())\n        all_protein_ids_full.extend(protein_ids)\n        \n        # Memory cleanup\n        del input_ids, attention_mask, logits, probs, encoded\n        \n        if batch_idx % 100 == 0:\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Progress update every 10k proteins\n        if (batch_idx * 4) % 10000 == 0:\n            print(f\"   Processed {batch_idx * 4:,} / {len(test_df_full):,} proteins...\")\n\n# Combine predictions\ntest_predictions_full = np.vstack(all_predictions_full)\nprint(f\"\\n‚úÖ Predictions complete!\")\nprint(f\"   Shape: {test_predictions_full.shape}\")\nprint(f\"   Proteins: {len(all_protein_ids_full):,}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ CELL 5 COMPLETE - All predictions generated!\")\nprint(\"=\"*70)\nprint(\"\\nNext: Run CELL 6 (Create Submission)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:05:43.063160Z","iopub.execute_input":"2025-11-18T19:05:43.063811Z","iopub.status.idle":"2025-11-18T20:45:51.937739Z","shell.execute_reply.started":"2025-11-18T19:05:43.063788Z","shell.execute_reply":"2025-11-18T20:45:51.936707Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîÆ CELL 5: GENERATING PREDICTIONS FOR ALL TEST PROTEINS\n======================================================================\n\nüì• Loading FULL test superset...\n‚ö†Ô∏è Loading ALL 224,309 proteins (not just 5,000)\n‚úÖ Loaded 224,309 test proteins\n‚úÖ Test loader ready: 224,309 proteins\n\nüîÆ Generating predictions...\n‚è±Ô∏è This will take 60-90 minutes - be patient!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting:   0%|          | 0/56078 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c190adb51f47d2b1ef75ac170ee654"}},"metadata":{}},{"name":"stdout","text":"   Processed 0 / 224,309 proteins...\n   Processed 10,000 / 224,309 proteins...\n   Processed 20,000 / 224,309 proteins...\n   Processed 30,000 / 224,309 proteins...\n   Processed 40,000 / 224,309 proteins...\n   Processed 50,000 / 224,309 proteins...\n   Processed 60,000 / 224,309 proteins...\n   Processed 70,000 / 224,309 proteins...\n   Processed 80,000 / 224,309 proteins...\n   Processed 90,000 / 224,309 proteins...\n   Processed 100,000 / 224,309 proteins...\n   Processed 110,000 / 224,309 proteins...\n   Processed 120,000 / 224,309 proteins...\n   Processed 130,000 / 224,309 proteins...\n   Processed 140,000 / 224,309 proteins...\n   Processed 150,000 / 224,309 proteins...\n   Processed 160,000 / 224,309 proteins...\n   Processed 170,000 / 224,309 proteins...\n   Processed 180,000 / 224,309 proteins...\n   Processed 190,000 / 224,309 proteins...\n   Processed 200,000 / 224,309 proteins...\n   Processed 210,000 / 224,309 proteins...\n   Processed 220,000 / 224,309 proteins...\n\n‚úÖ Predictions complete!\n   Shape: (224309, 7873)\n   Proteins: 224,309\n\n======================================================================\n‚úÖ CELL 5 COMPLETE - All predictions generated!\n======================================================================\n\nNext: Run CELL 6 (Create Submission)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\"\"\"\nCELL 6 - CREATE SUBMISSION FILE\nThis creates the final submission using Top-K method\nTime: ~5-10 minutes\n\"\"\"\n\nprint(\"=\"*70)\nprint(\"üìù CELL 6: CREATING SUBMISSION FILE\")\nprint(\"=\"*70)\n\n# ----------------------------------------------------------------------------\n# 6.1: Top-K submission creator\n# ----------------------------------------------------------------------------\n\ndef create_submission_topk(protein_ids, predictions, go_terms, k=50, output_file='submission.tsv'):\n    \"\"\"Create submission by selecting top K terms per protein\"\"\"\n    print(f\"\\nüìä Creating Top-{k} submission for {len(protein_ids):,} proteins...\")\n    \n    rows = []\n    for i in tqdm(range(len(protein_ids)), desc=\"Building submission\"):\n        pid = protein_ids[i]\n        probs = predictions[i]\n        \n        # Get top k predictions\n        if k >= len(probs):\n            topk_idx = np.argsort(probs)[::-1]\n        else:\n            topk_idx = np.argpartition(probs, -k)[-k:]\n            topk_idx = topk_idx[np.argsort(probs[topk_idx])][::-1]\n        \n        # Add to submission\n        for j in topk_idx:\n            rows.append(f\"{pid}\\t{go_terms[j]}\\t{probs[j]:.6f}\\n\")\n    \n    # Write to file\n    with open(output_file, 'w') as f:\n        f.writelines(rows)\n    \n    print(f\"‚úÖ Saved: {output_file}\")\n    print(f\"   Total predictions: {len(rows):,}\")\n    print(f\"   Proteins: {len(protein_ids):,}\")\n    print(f\"   Predictions per protein: {k}\")\n    \n    return output_file\n\n# ----------------------------------------------------------------------------\n# 6.2: Generate submissions\n# ----------------------------------------------------------------------------\n\nprint(\"\\n1Ô∏è‚É£ Creating Top-50 submission (RECOMMENDED):\")\nsubmission_50 = create_submission_topk(\n    all_protein_ids_full,\n    test_predictions_full,\n    all_go_terms,\n    k=50,\n    output_file='/kaggle/working/submission_full_top50.tsv'\n)\n\nprint(\"\\n2Ô∏è‚É£ Creating Top-100 submission:\")\nsubmission_100 = create_submission_topk(\n    all_protein_ids_full,\n    test_predictions_full,\n    all_go_terms,\n    k=100,\n    output_file='/kaggle/working/submission_full_top100.tsv'\n)\n\nprint(\"\\n3Ô∏è‚É£ Creating Top-20 submission:\")\nsubmission_20 = create_submission_topk(\n    all_protein_ids_full,\n    test_predictions_full,\n    all_go_terms,\n    k=20,\n    output_file='/kaggle/working/submission_full_top20.tsv'\n)\n\n# ----------------------------------------------------------------------------\n# 6.3: Validate submissions\n# ----------------------------------------------------------------------------\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ VALIDATION\")\nprint(\"=\"*70)\n\nimport os\n\nfor k, filepath in [(50, submission_50), (100, submission_100), (20, submission_20)]:\n    size_mb = os.path.getsize(filepath) / 1e6\n    print(f\"\\nTop-{k}: {filepath}\")\n    print(f\"  Size: {size_mb:.1f} MB\")\n    print(f\"  Expected predictions: {len(all_protein_ids_full) * k:,}\")\n\n# Sample check\nprint(\"\\nSample from Top-50:\")\nwith open(submission_50, 'r') as f:\n    for i, line in enumerate(f):\n        if i < 5:\n            print(f\"  {line.strip()}\")\n        else:\n            break\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéâ PIPELINE COMPLETE!\")\nprint(\"=\"*70)\n\nprint(\"\\nüì§ SUBMISSION FILES READY:\")\nprint(\"   1. submission_full_top50.tsv  ‚Üê SUBMIT THIS FIRST\")\nprint(\"   2. submission_full_top100.tsv\")\nprint(\"   3. submission_full_top20.tsv\")\n\nprint(\"\\nüìä WHAT CHANGED:\")\nprint(f\"   OLD: 5,000 proteins, 1.9 predictions per protein ‚Üí Score: 0.00\")\nprint(f\"   NEW: {len(all_protein_ids_full):,} proteins, 50 predictions per protein ‚Üí Should be >0!\")\n\nprint(\"\\nüí° EXPECTED RESULTS:\")\nprint(\"   ‚úÖ Non-zero score (almost guaranteed)\")\nprint(\"   ‚úÖ Full test set coverage\")\nprint(\"   ‚úÖ No threshold issues\")\n\nprint(\"\\nüîÑ TO IMPROVE FURTHER:\")\nprint(\"   1. Set USE_SUBSET=False in Cell 2 (train on full data)\")\nprint(\"   2. Increase NUM_EPOCHS to 5-10 in Cell 4\")\nprint(\"   3. Use larger model: esm2_t33_650M_UR50D\")\nprint(\"   4. Reduce MIN_FREQ to 5 (more GO terms)\")\nprint(\"   5. Tune per-term thresholds on validation set\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ READY TO SUBMIT! Download submission_full_top50.tsv\")\nprint(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T20:46:40.683853Z","iopub.execute_input":"2025-11-18T20:46:40.684434Z","iopub.status.idle":"2025-11-18T20:48:25.429886Z","shell.execute_reply.started":"2025-11-18T20:46:40.684412Z","shell.execute_reply":"2025-11-18T20:48:25.429231Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìù CELL 6: CREATING SUBMISSION FILE\n======================================================================\n\n1Ô∏è‚É£ Creating Top-50 submission (RECOMMENDED):\n\nüìä Creating Top-50 submission for 224,309 proteins...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Building submission:   0%|          | 0/224309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3308bd5bbd484b659804db2e58cfc45c"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Saved: /kaggle/working/submission_full_top50.tsv\n   Total predictions: 11,215,450\n   Proteins: 224,309\n   Predictions per protein: 50\n\n2Ô∏è‚É£ Creating Top-100 submission:\n\nüìä Creating Top-100 submission for 224,309 proteins...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Building submission:   0%|          | 0/224309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7e216ecf754f0781aacf302e74f4f8"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Saved: /kaggle/working/submission_full_top100.tsv\n   Total predictions: 22,430,900\n   Proteins: 224,309\n   Predictions per protein: 100\n\n3Ô∏è‚É£ Creating Top-20 submission:\n\nüìä Creating Top-20 submission for 224,309 proteins...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Building submission:   0%|          | 0/224309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18d480ce49c481b992804ace46755a3"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Saved: /kaggle/working/submission_full_top20.tsv\n   Total predictions: 4,486,180\n   Proteins: 224,309\n   Predictions per protein: 20\n\n======================================================================\n‚úÖ VALIDATION\n======================================================================\n\nTop-50: /kaggle/working/submission_full_top50.tsv\n  Size: 303.7 MB\n  Expected predictions: 11,215,450\n\nTop-100: /kaggle/working/submission_full_top100.tsv\n  Size: 607.4 MB\n  Expected predictions: 22,430,900\n\nTop-20: /kaggle/working/submission_full_top20.tsv\n  Size: 121.5 MB\n  Expected predictions: 4,486,180\n\nSample from Top-50:\n  A0A0C5B5G6\tGO:0005576\t0.367918\n  A0A0C5B5G6\tGO:0090729\t0.146563\n  A0A0C5B5G6\tGO:0007218\t0.049120\n  A0A0C5B5G6\tGO:0005739\t0.045457\n  A0A0C5B5G6\tGO:0005829\t0.039683\n\n======================================================================\nüéâ PIPELINE COMPLETE!\n======================================================================\n\nüì§ SUBMISSION FILES READY:\n   1. submission_full_top50.tsv  ‚Üê SUBMIT THIS FIRST\n   2. submission_full_top100.tsv\n   3. submission_full_top20.tsv\n\nüìä WHAT CHANGED:\n   OLD: 5,000 proteins, 1.9 predictions per protein ‚Üí Score: 0.00\n   NEW: 224,309 proteins, 50 predictions per protein ‚Üí Should be >0!\n\nüí° EXPECTED RESULTS:\n   ‚úÖ Non-zero score (almost guaranteed)\n   ‚úÖ Full test set coverage\n   ‚úÖ No threshold issues\n\nüîÑ TO IMPROVE FURTHER:\n   1. Set USE_SUBSET=False in Cell 2 (train on full data)\n   2. Increase NUM_EPOCHS to 5-10 in Cell 4\n   3. Use larger model: esm2_t33_650M_UR50D\n   4. Reduce MIN_FREQ to 5 (more GO terms)\n   5. Tune per-term thresholds on validation set\n\n======================================================================\n‚úÖ READY TO SUBMIT! Download submission_full_top50.tsv\n======================================================================\n","output_type":"stream"}],"execution_count":15}]}